{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7bfac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain.text_splitter.RecursiveCharacterTextSplitter object at 0x7fe23383cfd0>\n",
      "page_content='THE HOLY BIBLETRANSLATED FROM THE LATIN VULGATEDILIGENTLY COMPARED WITH THE HEBREW, GREEK, ANDOTHER EDITIONS IN DIVERS LANGUAGESDOUAY-RHEIMS VERSION1609, 1582' metadata={'source': '/Users/general/Downloads/bible_text.pdf', 'page': 0}\n",
      "4675\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from langchain.document_loaders import ReadTheDocsLoader,PyPDFLoader,DataFrameLoader # It will basically take the documnetations and convert it into the lagchain based docs.\n",
    "# Read the docs loader is useful for documentation reading.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "\n",
    "pinecone.init(\"4f028635-3471-4227-9e8b-d3b876dbfa37\",environment=\"asia-southeast1-gcp-free\") \n",
    "\n",
    "\n",
    "def ingest_doc():\n",
    "    loader =PyPDFLoader(r'/Users/general/Downloads/bible_text.pdf')\n",
    "    raw_doc=loader.load()\n",
    "    splitted_doc = RecursiveCharacterTextSplitter(chunk_size=1200,chunk_overlap=100,separators=[\"\\n\\n\",'\\n',' ',''])\n",
    "    print(splitted_doc)\n",
    "    raw_doc= splitted_doc.split_documents(raw_doc)  \n",
    "    print(raw_doc[0])  \n",
    "    print(len(raw_doc))\n",
    "    embeddings= OpenAIEmbeddings(openai_api_key='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')  \n",
    "\n",
    "    Pinecone.from_documents(raw_doc,embeddings,index_name='bible-chat')# currently revoked index_name # this will basically takes the chunks of the data and make use if openai embeddings to\n",
    "\n",
    "ingest_doc() \n",
    "\n",
    "# I Have successfully transferred the vectors in the pinecone \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3b74386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain import OpenAI\n",
    "\n",
    "pinecone.init(\"4f028635-3471-4227-9e8b-d3b876dbfa37\",environment=\"asia-southeast1-gcp-free\") \n",
    "\n",
    "\n",
    "# The only reason we have api's keys have been there is because of strem lit other wise we could make use of the \n",
    "# .env varibales\n",
    "\n",
    "index_name = 'bible-chat'  \n",
    "\n",
    "def run_llm(query):\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key='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')\n",
    "    docsearch = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings) \n",
    "    chat= ChatOpenAI(openai_api_key='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',verbose=True,temperature=0)\n",
    "    qa = RetrievalQA.from_llm(llm=chat,retriever=docsearch.as_retriever())\n",
    "    return qa({'query':query})   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f7890f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'can you provide me with answers from the holy quran?',\n",
       " 'result': \"I apologize, but I don't have access to the Holy Quran.\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_llm('can you provide me with answers from the holy quran?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e9f6468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
